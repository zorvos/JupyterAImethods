{"cells":[{"cell_type":"code","metadata":{"source_hash":"fe50a56a","execution_start":1724409585693,"execution_millis":9723,"deepnote_to_be_reexecuted":false,"cell_id":"e8451d9b8aac43b29a96b67960f96d88","deepnote_cell_type":"code"},"source":"pip install transformers torch\n","block_group":"e8451d9b8aac43b29a96b67960f96d88","execution_count":1,"outputs":[{"name":"stdout","text":"Collecting transformers\n  Downloading transformers-4.44.2-py3-none-any.whl (9.5 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.5/9.5 MB\u001b[0m \u001b[31m141.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: torch in /shared-libs/python3.9/py/lib/python3.9/site-packages (1.12.1)\nRequirement already satisfied: numpy>=1.17 in /shared-libs/python3.9/py/lib/python3.9/site-packages (from transformers) (1.23.4)\nCollecting tokenizers<0.20,>=0.19\n  Downloading tokenizers-0.19.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m154.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting safetensors>=0.4.1\n  Downloading safetensors-0.4.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (436 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m436.3/436.3 kB\u001b[0m \u001b[31m72.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /shared-libs/python3.9/py/lib/python3.9/site-packages (from transformers) (4.64.1)\nCollecting huggingface-hub<1.0,>=0.23.2\n  Downloading huggingface_hub-0.24.6-py3-none-any.whl (417 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m417.5/417.5 kB\u001b[0m \u001b[31m78.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: packaging>=20.0 in /shared-libs/python3.9/py-core/lib/python3.9/site-packages (from transformers) (21.3)\nCollecting pyyaml>=5.1\n  Downloading PyYAML-6.0.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (737 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m737.4/737.4 kB\u001b[0m \u001b[31m58.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: requests in /shared-libs/python3.9/py/lib/python3.9/site-packages (from transformers) (2.28.1)\nRequirement already satisfied: filelock in /shared-libs/python3.9/py/lib/python3.9/site-packages (from transformers) (3.8.0)\nRequirement already satisfied: regex!=2019.12.17 in /shared-libs/python3.9/py/lib/python3.9/site-packages (from transformers) (2022.9.13)\nRequirement already satisfied: typing-extensions in /shared-libs/python3.9/py/lib/python3.9/site-packages (from torch) (4.4.0)\nRequirement already satisfied: fsspec>=2023.5.0 in /shared-libs/python3.9/py/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.2.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /shared-libs/python3.9/py-core/lib/python3.9/site-packages (from packaging>=20.0->transformers) (3.0.9)\nRequirement already satisfied: certifi>=2017.4.17 in /shared-libs/python3.9/py/lib/python3.9/site-packages (from requests->transformers) (2022.9.24)\nRequirement already satisfied: idna<4,>=2.5 in /shared-libs/python3.9/py-core/lib/python3.9/site-packages (from requests->transformers) (3.4)\nRequirement already satisfied: charset-normalizer<3,>=2 in /shared-libs/python3.9/py-core/lib/python3.9/site-packages (from requests->transformers) (2.1.1)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /shared-libs/python3.9/py/lib/python3.9/site-packages (from requests->transformers) (1.26.12)\nInstalling collected packages: safetensors, pyyaml, huggingface-hub, tokenizers, transformers\nSuccessfully installed huggingface-hub-0.24.6 pyyaml-6.0.2 safetensors-0.4.4 tokenizers-0.19.1 transformers-4.44.2\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"outputs_reference":"s3:deepnote-cell-outputs-production/80542c99-3850-4f86-b74a-e93a6719e666","content_dependencies":null},{"cell_type":"code","metadata":{"source_hash":"2cba00e6","execution_start":1724409619341,"execution_millis":4234,"deepnote_to_be_reexecuted":false,"cell_id":"afd1f2355e0d4bd28274f8c061543f91","deepnote_cell_type":"code"},"source":"from transformers import GPT2LMHeadModel, GPT2Tokenizer\n\n# Load pre-trained model and tokenizer\nmodel_name = 'gpt2'  # You can use 'gpt2-medium', 'gpt2-large', etc., for larger models\nmodel = GPT2LMHeadModel.from_pretrained(model_name)\ntokenizer = GPT2Tokenizer.from_pretrained(model_name)","block_group":"721349c7b6b041d8ba53fe6c51bcbd78","execution_count":2,"outputs":[{"name":"stderr","text":"/shared-libs/python3.9/py/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n/usr/local/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"}],"outputs_reference":"dbtable:cell_outputs/be5f8a34-7414-43e7-948f-84a011db5732","content_dependencies":null},{"cell_type":"code","metadata":{"source_hash":"c36cf61a","execution_start":1724409645113,"execution_millis":20945,"deepnote_to_be_reexecuted":false,"cell_id":"264d1c3204ae4c0785db342d22b8d92b","deepnote_cell_type":"code"},"source":"def generate_text(prompt, max_length=50, num_return_sequences=1):\n    # Encode the prompt text to get input IDs\n    input_ids = tokenizer.encode(prompt, return_tensors='pt')\n    \n    # Generate text using the model\n    outputs = model.generate(\n        input_ids,\n        max_length=max_length,\n        num_return_sequences=num_return_sequences,\n        no_repeat_ngram_size=2,  # Prevent repeating n-grams\n        early_stopping=True\n    )\n    \n    # Decode the generated text\n    generated_text = [tokenizer.decode(output, skip_special_tokens=True) for output in outputs]\n    \n    return generated_text\n\n# Generate sample text\nprompt = \"Once upon a time\"\ngenerated_texts = generate_text(prompt, max_length=100, num_return_sequences=1)\nprint(generated_texts[0])\n","block_group":"bf58b61a0072467b9cf46a81076e2bc1","execution_count":3,"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:615: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n  warnings.warn(\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nThe attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nOnce upon a time, the world was a place of great beauty and great danger. The world of the gods was the place where the great gods were born, and where they were to live.\n\nThe world that was created was not the same as the one that is now. It was an endless, endless world. And the Gods were not born of nothing. They were created of a single, single thing. That was why the universe was so beautiful. Because the cosmos was made of two\n","output_type":"stream"}],"outputs_reference":"s3:deepnote-cell-outputs-production/273e196b-4012-47f1-b9df-d0e2a3b74cef","content_dependencies":null},{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"9cd560d08cad448c970f228346ce0f41","deepnote_cell_type":"text-cell-p"},"source":"","block_group":"f793d456f9834513b0514504d05c5577"},{"cell_type":"markdown","metadata":{"formattedRanges":[{"type":"marks","marks":{"bold":true},"toCodePoint":23,"fromCodePoint":0}],"cell_id":"bf190c482861440eb2fad06616c59bf7","deepnote_cell_type":"text-cell-p"},"source":" Run the Notebook Cells","block_group":"cfa245d37ebe4ed29841d5303f82ccb7"},{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"c414c472bd9744b7af0d49af3f6fac32","deepnote_cell_type":"text-cell-p"},"source":"Execute each cell in your Jupyter notebook to see the results. The generate_text function takes a prompt and generates a continuation based on the pre-trained GPT-2 model.\nKey Points","block_group":"4298ca02df06439da2f01212c7518d71"},{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"be21e9c8df654093b9ed53977f2452f6","deepnote_cell_type":"text-cell-p"},"source":"    Model Choice: gpt2 is a smaller, less resource-intensive model. If you need more power, consider using gpt2-medium, gpt2-large, or gpt2-xl.","block_group":"5489d8715a504670a75b1d8e4bc7a01a"},{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"d72abb4faa6c4cd0b8a2e4b434bcb3f6","deepnote_cell_type":"text-cell-p"},"source":"    Text Generation Parameters:\n        max_length: Controls the length of the generated text.\n        num_return_sequences: Number of text sequences to return.\n        no_repeat_ngram_size: Helps avoid repeating phrases by limiting the size of repeated n-grams.","block_group":"caa5f40ca84d4778ae9adca375ebbd91"},{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"f12177227cdb454984f23a941a4d14d1","deepnote_cell_type":"text-cell-p"},"source":"    Performance: The larger the model, the better the results, but also the higher the computational requirements.","block_group":"cc94a3be2c804bfe9b1723d333530631"},{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"1bb185b778cc41b782c3796c35321995","deepnote_cell_type":"text-cell-p"},"source":"This setup provides a good balance between simplicity and functionality, making it a great starting point for experimenting with language models in Python.","block_group":"7f09dd1ac7854e7cb40db7cadf433e19"},{"cell_type":"markdown","source":"<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=7ca93078-354e-4f8a-a418-4655f32251b5' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>","metadata":{"created_in_deepnote_cell":true,"deepnote_cell_type":"markdown"}}],"nbformat":4,"nbformat_minor":0,"metadata":{"deepnote_notebook_id":"2a40032a5701469ca3960670005a18a1","deepnote_execution_queue":[]}}